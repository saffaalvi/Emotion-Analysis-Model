<!DOCTYPE html>
<html lang="en">
<head>
  <title>Dataset</title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  <link rel="stylesheet" href="{{ url_for('static', filename= 'css/style.css') }}">
  <link href="https://fonts.googleapis.com/css?family=Montserrat" rel="stylesheet">
  <style>
    
    body {
    font: 20px Montserrat, sans-serif;
    line-height: 1.8;
    color: #030c15;
    }

    .container-fluid {
    padding-top: 0px;
    padding-bottom: 0px;
    }

    .container-footer {
    padding-top: 70px;
    padding-bottom: 70px;
    }

    /* Remove the navbar's default margin-bottom and rounded borders */ 
    .navbar {
      margin-bottom: 0;
      border-radius: 0;
    }
    
    /* Set height of the grid so .sidenav can be 100% (adjust as needed) */
    .row.content {height: 450px}
    
    /* Set gray background color and 100% height */
    .sidenav {
      padding-top: 20px;
      background-color: #f1f1f1;
      height: 350%;
    }

    .setpadding {
      padding-left:25px
    }
    
    /* Set black background color, white text and some padding */
    footer {
      background-color: #555;
      color: white;
      padding: 15px;
    }
    
    /* On small screens, set height to 'auto' for sidenav and grid */
    @media screen and (max-width: 767px) {
      .sidenav {
        height: auto;
        padding: 15px;
      }
      .row.content {height:auto;} 
    }

    p {
      font-size: 22px;
    }

    .small {
      font-size: 14px;
    }

  </style>
</head>
<body>

<nav class="navbar navbar-default">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#myNavbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>                        
      </button>
      <a class="navbar-brand" href="/">SmartCampus</a>
    </div>
    <div class="collapse navbar-collapse" id="myNavbar">
      <ul class="nav navbar-nav navbar-right">
        <li><a href="https://github.com/saffaalvi/Emotion-Analysis-Model">GitHub</a></li>
        <li><a href="https://www.uwindsor.ca/">University of Windsor</a></li>
      </ul>
    </div>
  </div>
</nav>
  
<div class="container-fluid text-left">
  <div class="row content">
      <div class="col-sm-2 sidenav">
          <li><a href="/model">CNN</a></li>
          <li><a href="/notebook">Jupyter Notebook</a></li>
          <li><a href="/dataset">Dataset - Ravdess</a></li>
          <li><a href="/analytics">Model Analytics</a></li>
      </div>
      <div class="col-sm-8 setpadding text-left">
          <h1>Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS)</h1>
          <p>
              The dataset we used in order to train and test our model is the Ryerson Audio-Visual Database of Emotional Speech and Song dataset (or RAVDESS). This dataset contains 7356 files of 12 female and 12 male voices, vocalizing two statements in a neutral North
              American accent. Each of the 24 actors records 2 statements in each of 7 emotions which include calm, happy, sad, angry, fearful, surprise, and disgust. The audio also provides two levels of emotional intensity (normal, strong), with
              an additional neutral expression.
          </p>
          <p>
              The filenames of each audio file consist of numerical identifiers, representing different characteristics:
              <ol>
                  <li>Modality: audio files, video files or both</li>
                  <li>Vocal Channel: speech or song audio files</li>
                  <li>Emotion: normal, calm, happy, sad, angry, fearful, disgust, surprised</li>
                  <li>Emotional Intensity: normal or strong</li>
                  <li>Sentence: 01 = “Kids are talking by the door”, 02 = “Dogs are sitting by the door”</li>
                  <li>Sentence Repetition</li>
                  <li>Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).</li>
              </ol>
              For the purposes of this project, audio-only files were used as data input (modality = 03), and only speech files were used (voice channel = 01).
          </p>
          <p>When loading the dataset onto the model, the audio file names were broken down by identifier in order to obtain variables in each file, such as gender, actor and emotion. After retrieving the emotion identifiers, each one was redefined:
              <ul>
                  <li>1 = neutral</li>
                  <li>2 = calm</li>
                  <li>3 = happy</li>
                  <li>4 = sad</li>
                  <li>5 = angry</li>
                  <li>6 = fearful</li>
                  <li>7 = disgust</li>
                  <li>8 = surprised</li>
              </ul>
          </p>
          <hr>
          <p> For more detailed information, please visit the RAVDESS website: <a href="https://smartlaboratory.org/ravdess/">https://smartlaboratory.org/ravdess/</a> </p> <br>
          <p class="small"> Livingstone SR, Russo FA (2018) The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in North American English. PLoS ONE 13(5): e0196391. https://doi.org/10.1371/journal.pone.0196391. </p>
        </div>
  </div>
</div>

<!-- Footer -->
<footer class="container-footer bg-4 text-center">
    <p>Bootstrap Theme Made By <a href="https://www.w3schools.com">www.w3schools.com</a></p>
    <p>Created By Saffa Alvi and Nour ElKott</p>   
  </footer>
  
  </body>
  </html>
  